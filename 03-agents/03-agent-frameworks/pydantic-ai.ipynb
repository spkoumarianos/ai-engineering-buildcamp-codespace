{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f8a7f-9d2e-4708-b764-69a7e4b02235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524acdc1-c9b1-4a76-8ba5-24e4fcc39c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x250e75d0440>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    repo_name=\"docs\",\n",
    "    allowed_extensions={\"md\", \"mdx\"},\n",
    ")\n",
    "files = reader.read()\n",
    "\n",
    "parsed_docs = [doc.parse() for doc in files]\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "index.fit(parsed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b15f74-0f99-46b3-ab05-dca20f98c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Highlighter, Tokenizer\n",
    "from minsearch.tokenizer import DEFAULT_ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a45d140-6216-45c5-8207-6202a75e0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = DEFAULT_ENGLISH_STOP_WORDS | {'evidently'}\n",
    "\n",
    "highlighter = Highlighter(\n",
    "    highlight_fields=['content'],\n",
    "    max_matches=3,\n",
    "    snippet_size=50,\n",
    "    tokenizer=Tokenizer(stemmer='snowball', stop_words=stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ef72be-c145-4d7b-b4cb-6e6501a26f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {doc['filename']: doc['content'] for doc in parsed_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026bad79-0bef-406e-8b33-e49812c67964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "class SearchTools:\n",
    "    \"\"\"\n",
    "    Provides search and file retrieval utilities over an indexed data store.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: Any,\n",
    "        highlighter: Any,\n",
    "        file_index: Dict[str, str]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SearchTools instance.\n",
    "\n",
    "        Args:\n",
    "            index: Searchable index providing a `search` method.\n",
    "            highlighter: Highlighter used to annotate search results.\n",
    "            file_index (Dict[str, str]): Mapping of filenames to file contents.\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.highlighter = highlighter\n",
    "        self.file_index = file_index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the index for results matching a query and highlight them.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query to look up in the index.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of highlighted search result objects.\n",
    "        \"\"\"\n",
    "        search_results = self.index.search(\n",
    "            query=query,\n",
    "            num_results=5\n",
    "        )\n",
    "\n",
    "        return self.highlighter.highlight(query, search_results)\n",
    "\n",
    "    def get_file(self, filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve a file's contents by filename.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the file to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The file contents if found, otherwise an error message.\n",
    "        \"\"\"\n",
    "        if filename in self.file_index:\n",
    "            return self.file_index[filename]\n",
    "        return f\"file {filename} does not exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2031f343-3a0b-4637-92fa-fff908cdf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index, highlighter, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684bc3ef-47c1-44e3-a71b-6117a3368e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a documentation assistant.\n",
    "\n",
    "Answer the user question using only the documentation knowledge base.\n",
    "\n",
    "Make 3 iterations:\n",
    "\n",
    "1) First iteration:\n",
    "   - Perform one search using the search tool to identify potentially relevant documents.\n",
    "   - Explain (in 2–3 sentences) why this search query is appropriate for the user question.\n",
    "\n",
    "2) Second iteration:\n",
    "   - Analyze the results from the previous search.\n",
    "   - Based on the filenames or documents returned, perform:\n",
    "       - Up to 2 additional search queries to refine or expand coverage, and\n",
    "       - One or more get_file calls to retrieve the full content of the most relevant documents.\n",
    "   - For each search or get_file call, explain (in 2–3 sentences) why this action is necessary and how it helps answer the question.\n",
    "\n",
    "3) Third iteration:\n",
    "   - Analyze the retrieved document contents from get_file.\n",
    "   - Synthesize the information from these documents into a final answer to the user.\n",
    "\n",
    "IMPORTANT:\n",
    "- At every step, explicitly explain your reasoning for each search query or file retrieval.\n",
    "- Use only facts found in the documentation knowledge base.\n",
    "- Do not introduce outside knowledge or assumptions.\n",
    "- If the answer cannot be found in the retrieved documents, clearly inform the user.\n",
    "\n",
    "Additional notes:\n",
    "- The knowledge base is entirely about Evidently, so you do not need to include the word \"evidently\" in search queries.\n",
    "- Prefer retrieving and analyzing full documents (via get_file) before producing the final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06709c88-0523-462b-9562-c24dc923e673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b160af5-3f1a-445d-a638-17da66d94bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from toyaikit.tools import get_instance_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4602a5c9-8fd1-45e4-8d90-068e966d35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [search_tools.search, search_tools.get_file]\n",
    "tools = get_instance_methods(search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea8f1197-f476-4ecf-8fb4-7ceb3a015726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    model='openai:gpt-4o-mini',\n",
    "    instructions=instructions,\n",
    "    tools=tools\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85fd22-edf4-453a-8a5e-a9ee9662f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f91a7-9afe-4429-ba29-ac5b060e3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I use evidently to monitor my machine learning models?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c33403-a90c-4af5-977c-5d10dc4a53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await search_agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803a1c47-3ba3-4352-aad4-6a0d57c7a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) From the retrieved documents, I found the following relevant information about how to use Evidently to monitor your machine learning models.\n",
      "\n",
      "Evidently provides several options for monitoring machine learning models effectively. The platform supports batch monitoring jobs, which is suitable for batch ML pipelines, regression testing, and near real-time ML systems. You can create an evaluation pipeline using tools such as Python scripts or orchestration tools like Airflow. This involves scheduling evaluations, running metric calculations, and visualizing the results on a dashboard. \n",
      "\n",
      "Additionally, Evidently also offers tracing capabilities for applications powered by large language models (LLMs). This method involves instrumenting your application to capture relevant data and scheduling evaluations that can run automatically. This makes data management simpler and allows for easier re-evaluation of model performance.\n",
      "\n",
      "**Key Steps to Monitor Your Machine Learning Models Using Evidently:**\n",
      "1. **Set Up Monitoring Jobs:** Create an evaluation pipeline that runs at regular intervals using the Evidently Python library for metric calculations and reporting.\n",
      "2. **Use Batch Monitoring for Regular Evaluation:** Ideal for scenarios where data arrives periodically, such as daily or hourly.\n",
      "3. **Implement Tracing for LLM Applications:** Capture inputs and outputs efficiently with scheduled evaluations to facilitate easy retrieval and re-evaluation.\n",
      "\n",
      "For more detailed guidance and to get started quickly, you can refer to the specific quickstart documentation for ML monitoring and LLM evaluation linked in the overview document. This will provide you with practical examples and further insights into using the platform effectively.\n"
     ]
    }
   ],
   "source": [
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39b614a-1789-40bb-90e2-e50ce085a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = result.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9cf03aa-2b56-4acf-8dc6-ea30696acf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunUsage(input_tokens=4031, output_tokens=571, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, requests=3, tool_calls=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b41bc144-2fdf-42e6-9a50-949036429448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "USER: how do I use evidently to monitor my machine learning models?\n",
      "\n",
      "response\n",
      "TOOL CALL: search {\"query\":\"monitor machine learning models\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: search\n",
      "\n",
      "response\n",
      "1) I searched for \"monitor machine learning models\" because it directly relates to your inquiry about using Evidently for monitoring machine learning models. This search should yield relevant documents about monitoring capabilities and practices within the tools or frameworks provided by Evidently.\n",
      "\n",
      "2) Analyzing the search results, the document titled \"Overview\" discusses how production AI quality monitoring works, which likely includes procedures and examples relevant to monitoring models. I'll perform a get_file call on this document to retrieve its contents.\n",
      "\n",
      "Additionally, the document titled \"What is Evidently?\" may contain an overview of monitoring and evaluation tools within the platform. I'll perform another get_file call to access this document.\n",
      "\n",
      "Here’s the request to retrieve the contents of these documents:\n",
      "\n",
      "- Get file for \"docs/platform/monitoring_overview.mdx\"\n",
      "- Get file for \"introduction.mdx\" \n",
      "\n",
      "These actions will provide comprehensive details on monitoring usage and its foundational context, helping to answer your question effectively. \n",
      "\n",
      "Let's proceed with these requests.\n",
      "TOOL CALL: get_file {\"filename\": \"docs/platform/monitoring_overview.mdx\"}\n",
      "TOOL CALL: get_file {\"filename\": \"introduction.mdx\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: get_file\n",
      "TOOL RETURN: get_file\n",
      "\n",
      "response\n",
      "3) From the retrieved documents, I found the following relevant information about how to use Evidently to monitor your machine learning models.\n",
      "\n",
      "Evidently provides several options for monitoring machine learning models effectively. The platform supports batch monitoring jobs, which is suitable for batch ML pipelines, regression testing, and near real-time ML systems. You can create an evaluation pipeline using tools such as Python scripts or orchestration tools like Airflow. This involves scheduling evaluations, running metric calculations, and visualizing the results on a dashboard. \n",
      "\n",
      "Additionally, Evidently also offers tracing capabilities for applications powered by large language models (LLMs). This method involves instrumenting your application to capture relevant data and scheduling evaluations that can run automatically. This makes data management simpler and allows for easier re-evaluation of model performance.\n",
      "\n",
      "**Key Steps to Monitor Your Machine Learning Models Using Evidently:**\n",
      "1. **Set Up Monitoring Jobs:** Create an evaluation pipeline that runs at regular intervals using the Evidently Python library for metric calculations and reporting.\n",
      "2. **Use Batch Monitoring for Regular Evaluation:** Ideal for scenarios where data arrives periodically, such as daily or hourly.\n",
      "3. **Implement Tracing for LLM Applications:** Capture inputs and outputs efficiently with scheduled evaluations to facilitate easy retrieval and re-evaluation.\n",
      "\n",
      "For more detailed guidance and to get started quickly, you can refer to the specific quickstart documentation for ML monitoring and LLM evaluation linked in the overview document. This will provide you with practical examples and further insights into using the platform effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in messages:\n",
    "    print(m.kind)\n",
    "    for p in m.parts:\n",
    "        part_kind = p.part_kind\n",
    "        if part_kind == 'user-prompt':\n",
    "            print('USER:', p.content)\n",
    "        if part_kind == 'tool-call':\n",
    "            print('TOOL CALL:', p.tool_name, p.args)\n",
    "        if part_kind == 'tool-return':\n",
    "            print('TOOL RETURN:', p.tool_name)\n",
    "        if part_kind == 'text':\n",
    "            print(p.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cafce44f-d065-434f-bc82-f7eb7818bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = 'show me the code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ead89483-d67c-4580-b743-e2f8daeb1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = await search_agent.run(new_prompt, message_history=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fa4e6f2-43f8-4e56-8254-c0e8abc22dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, here are code examples to help you get started with using Evidently for monitoring machine learning models.\n",
      "\n",
      "### 1. Quickstart for Monitoring ML Models\n",
      "\n",
      "To monitor your machine learning models, you can follow a basic setup where you run evaluations and visualize results. Below is a sample code snippet that illustrates how to use Evidently for monitoring data quality and detecting data drift:\n",
      "\n",
      "```python\n",
      "from evidently import ColumnMapping\n",
      "from evidently.report import Report\n",
      "from evidently.runners import Runner\n",
      "\n",
      "# Load your data and reference\n",
      "reference_data = ...\n",
      "current_data = ...\n",
      "\n",
      "# Define column mapping\n",
      "column_mapping = ColumnMapping(\n",
      "    target=\"your_target_column_name\",\n",
      "    prediction=\"your_predictions_column_name\",\n",
      "    numerical_features=[\"numerical_feature_1\", \"numerical_feature_2\"],\n",
      "    categorical_features=[\"categorical_feature_1\"]\n",
      ")\n",
      "\n",
      "# Create a report\n",
      "report = Report()\n",
      "\n",
      "# Add metrics to the report\n",
      "report.add_metric(\"DataDrift\", current_data, reference_data, column_mapping)\n",
      "report.add_metric(\"DataQuality\", current_data, column_mapping)\n",
      "\n",
      "# Run the report\n",
      "report.run(current_data=current_data, reference_data=reference_data)\n",
      "\n",
      "# Visualize the report\n",
      "report.show()\n",
      "```\n",
      "\n",
      "### 2. Metric Cookbook\n",
      "\n",
      "You can find more detailed metrics and specific use cases in the \"Metric Cookbook.\" For example, here is how you would calculate and visualize specific metrics:\n",
      "\n",
      "```python\n",
      "from evidently.metrics import DataDriftPreset, ValueStats\n",
      "\n",
      "# Define your dataset\n",
      "data = ...\n",
      "\n",
      "# Run a data drift assessment\n",
      "drift_results = DataDriftPreset(data, reference=reference_data)\n",
      "drift_results.run()\n",
      "\n",
      "# Get value stats\n",
      "value_stats = ValueStats(data)\n",
      "value_results = value_stats.run()\n",
      "\n",
      "# Visualize results\n",
      "print(\"Data Drift Results:\", drift_results.to_json())\n",
      "print(\"Value Statistics:\", value_results)\n",
      "```\n",
      "\n",
      "These examples give you a pathway to monitor your machine learning models effectively using Evidently, ensuring that you're regularly evaluating their performance and checking for data quality issues.\n",
      "\n",
      "For detailed workflows and more code examples, you can explore the tutorials linked in the retrieved documents, including:\n",
      "\n",
      "- [LLM Quickstart](https://www.evidentlyai.com/llm-evaluation-course-practice)\n",
      "- [Metric Cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb)\n",
      "\n",
      "If you need specific examples for different metrics or scenarios like text evaluations or classification, refer to the individual sections for those use cases as provided in the documentation.\n"
     ]
    }
   ],
   "source": [
    "print(result2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00a4396a-4123-4c59-8db0-6ec17a1f2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(messages):\n",
    "    for m in messages:\n",
    "        print(m.kind)\n",
    "        for p in m.parts:\n",
    "            part_kind = p.part_kind\n",
    "            if part_kind == 'user-prompt':\n",
    "                print('USER:', p.content)\n",
    "            if part_kind == 'tool-call':\n",
    "                print('TOOL CALL:', p.tool_name, p.args)\n",
    "            if part_kind == 'tool-return':\n",
    "                print('TOOL RETURN:', p.tool_name)\n",
    "            if part_kind == 'text':\n",
    "                print(p.content)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12a3a29a-096f-404a-93ea-537d899842ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "USER: show me the code\n",
      "\n",
      "response\n",
      "TOOL CALL: search {\"query\":\"code examples\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: search\n",
      "\n",
      "response\n",
      "1) I searched for \"code examples\" to find specific coding implementations related to Evidently. This will help me locate documents that provide sample code or examples demonstrating how to use the library for monitoring machine learning models effectively.\n",
      "\n",
      "2) The search results indicate that the document titled \"Tutorials and guides\" contains end-to-end examples of specific workflows and will likely include relevant code snippets. Therefore, I will perform a get_file call for this document to retrieve its content.\n",
      "\n",
      "Additionally, the document titled \"All Metrics\" mentions reference code examples associated with dataset-level evaluations. I'll retrieve this document as well.\n",
      "\n",
      "Here’s the request to retrieve the contents of these two documents:\n",
      "\n",
      "- Get file for \"examples/introduction.mdx\"\n",
      "- Get file for \"metrics/all_metrics.mdx\"\n",
      "\n",
      "These documents should provide practical code snippets that will illustrate how to implement monitoring functionalities within the Evidently framework. \n",
      "\n",
      "Let's proceed with these requests.\n",
      "TOOL CALL: get_file {\"filename\": \"examples/introduction.mdx\"}\n",
      "TOOL CALL: get_file {\"filename\": \"metrics/all_metrics.mdx\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: get_file\n",
      "TOOL RETURN: get_file\n",
      "\n",
      "response\n",
      "Based on the retrieved documents, here are code examples to help you get started with using Evidently for monitoring machine learning models.\n",
      "\n",
      "### 1. Quickstart for Monitoring ML Models\n",
      "\n",
      "To monitor your machine learning models, you can follow a basic setup where you run evaluations and visualize results. Below is a sample code snippet that illustrates how to use Evidently for monitoring data quality and detecting data drift:\n",
      "\n",
      "```python\n",
      "from evidently import ColumnMapping\n",
      "from evidently.report import Report\n",
      "from evidently.runners import Runner\n",
      "\n",
      "# Load your data and reference\n",
      "reference_data = ...\n",
      "current_data = ...\n",
      "\n",
      "# Define column mapping\n",
      "column_mapping = ColumnMapping(\n",
      "    target=\"your_target_column_name\",\n",
      "    prediction=\"your_predictions_column_name\",\n",
      "    numerical_features=[\"numerical_feature_1\", \"numerical_feature_2\"],\n",
      "    categorical_features=[\"categorical_feature_1\"]\n",
      ")\n",
      "\n",
      "# Create a report\n",
      "report = Report()\n",
      "\n",
      "# Add metrics to the report\n",
      "report.add_metric(\"DataDrift\", current_data, reference_data, column_mapping)\n",
      "report.add_metric(\"DataQuality\", current_data, column_mapping)\n",
      "\n",
      "# Run the report\n",
      "report.run(current_data=current_data, reference_data=reference_data)\n",
      "\n",
      "# Visualize the report\n",
      "report.show()\n",
      "```\n",
      "\n",
      "### 2. Metric Cookbook\n",
      "\n",
      "You can find more detailed metrics and specific use cases in the \"Metric Cookbook.\" For example, here is how you would calculate and visualize specific metrics:\n",
      "\n",
      "```python\n",
      "from evidently.metrics import DataDriftPreset, ValueStats\n",
      "\n",
      "# Define your dataset\n",
      "data = ...\n",
      "\n",
      "# Run a data drift assessment\n",
      "drift_results = DataDriftPreset(data, reference=reference_data)\n",
      "drift_results.run()\n",
      "\n",
      "# Get value stats\n",
      "value_stats = ValueStats(data)\n",
      "value_results = value_stats.run()\n",
      "\n",
      "# Visualize results\n",
      "print(\"Data Drift Results:\", drift_results.to_json())\n",
      "print(\"Value Statistics:\", value_results)\n",
      "```\n",
      "\n",
      "These examples give you a pathway to monitor your machine learning models effectively using Evidently, ensuring that you're regularly evaluating their performance and checking for data quality issues.\n",
      "\n",
      "For detailed workflows and more code examples, you can explore the tutorials linked in the retrieved documents, including:\n",
      "\n",
      "- [LLM Quickstart](https://www.evidentlyai.com/llm-evaluation-course-practice)\n",
      "- [Metric Cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb)\n",
      "\n",
      "If you need specific examples for different metrics or scenarios like text evaluations or classification, refer to the individual sections for those use cases as provided in the documentation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_messages(result2.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db9be0d5-c0b9-44fb-a712-751c845d118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d40eb269-7d59-444f-856e-388173f2df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: dsasboard evidently\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "USER: dsasboard evidently\n",
      "\n",
      "response\n",
      "1) I will search for \"dashboard\" as it is a likely relevant term related to managing or visualizing data within the context of the platform. This search will help uncover documentation related to dashboard features or functionalities, which should inform the user regarding their inquiry.\n",
      "\n",
      "```json\n",
      "{\"query\": \"dashboard\"}\n",
      "```\n",
      "\n",
      "2) Now, I'll analyze the results from the previous search to determine their relevance. Based on the items returned, I will perform an additional search query for \"dashboard features\" to refine the results specifically around the functionalities available. Additionally, I will retrieve content from the document that seems most relevant to the dashboard topic to better address any specific information the user might be seeking.\n",
      "\n",
      "```json\n",
      "{\"query\": \"dashboard features\"}\n",
      "```\n",
      "\n",
      "3) Once the relevant documents are retrieved, I will read through their content to provide a comprehensive answer, synthesizing the information effectively based on what is presented in those documents.\n",
      "\n",
      "Let's proceed with executing the first search.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: do the actual search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "USER: do the actual search\n",
      "\n",
      "response\n",
      "TOOL CALL: search {\"query\":\"dashboard\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: search\n",
      "\n",
      "response\n",
      "The search for \"dashboard\" returned several relevant documents that provide insights into the features and functionalities related to dashboards. The documents seem to cover different aspects such as adding panels, an overview of dashboards, and legacy panel types. This breadth of results indicates that there is useful information that can be synthesized to answer the user’s question effectively.\n",
      "\n",
      "Next, I will perform an additional search for \"dashboard features\" to further hone in on specific functionalities, which may provide detailed explanations or guidelines relevant to dashboard capabilities. Additionally, I will retrieve content from the most relevant document titled \"Overview,\" which is crucial for understanding what a dashboard is and its core features.\n",
      "\n",
      "Let's execute these next actions.\n",
      "TOOL CALL: search {\"query\": \"dashboard features\"}\n",
      "TOOL CALL: get_file {\"filename\": \"docs/platform/dashboard_overview.mdx\"}\n",
      "\n",
      "request\n",
      "TOOL RETURN: search\n",
      "TOOL RETURN: get_file\n",
      "\n",
      "response\n",
      "In the second iteration, I retrieved the document titled \"Overview\" related to dashboards, which provides foundational information about their purpose and functionalities. This document highlights that dashboards give a clear view of AI application performance, allowing users to track various metrics across multiple experiments and monitor live production quality. It mentions the structure of dashboards, including tabs for organizing panels, and defines what panels are and how to add them using both Python API and UI methods.\n",
      "\n",
      "Additionally, I conducted an additional search for \"dashboard features,\" which yielded documents that likely provide more detailed specifics regarding the capabilities and configurations available for dashboards. Relevant documents include guides on adding dashboard panels via UI and API, as well as an overview of panel types.\n",
      "\n",
      "Now, I will move forward to analyze the retrieved content of the \"Overview\" document to synthesize a comprehensive answer to the user's question regarding dashboards. \n",
      "\n",
      "The key points from the \"Overview\" document include:\n",
      "- **Dashboard Definition**: A dashboard provides a visual overview of application performance, helps in tracking evaluation results and monitoring production quality.\n",
      "- **Structure**: Dashboards are organized with tabs where panels display specific metrics, allowing easy navigation and organization.\n",
      "- **Panel Types**: Panels are visual components that can represent data through various formats, such as counters or plots, and they can be customized based on user needs.\n",
      "- **Data Source**: Dashboards require reports as data sources, which means at least one report needs to be saved to populate the dashboard.\n",
      "\n",
      "Based on this synthesis, here is the final answer to your question about dashboards:\n",
      "\n",
      "### Final Answer:\n",
      "A dashboard is a visual interface that allows users to monitor the performance of AI applications by displaying metrics across various experiments and in real-time production. It consists of multiple panels that can show different types of data, such as counters and line plots. Users can organize these panels into tabs for clarity and can add them either through a user interface or programmatically via an API. Dashboards rely on reports from which they pull the necessary data for visualization. \n",
      "\n",
      "If you require more specific details or functionality related to dashboards, please let me know!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "usage = RunUsage()\n",
    "\n",
    "while True:\n",
    "    user_prompt = input('You:')\n",
    "    if user_prompt.lower().strip() == 'stop':\n",
    "        break\n",
    "\n",
    "    result = await search_agent.run(user_prompt, message_history=messages)\n",
    "    usage = usage + result.usage()\n",
    "\n",
    "    print_messages(result.new_messages())\n",
    "    messages.extend(result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "feba96e9-3ee1-4e70-9ceb-f42927ef0fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunUsage(input_tokens=4804, cache_read_tokens=1280, output_tokens=811, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, requests=4, tool_calls=3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1f1435c-d280-4470-9d7f-161f43b6c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import FunctionToolCallEvent\n",
    "\n",
    "class NamedCallback:\n",
    "\n",
    "    def __init__(self, agent):\n",
    "        self.agent_name = agent.name\n",
    "\n",
    "    async def print_function_calls(self, ctx, event):\n",
    "        # Detect nested streams\n",
    "        if hasattr(event, \"__aiter__\"):\n",
    "            async for sub in event:\n",
    "                await self.print_function_calls(ctx, sub)\n",
    "            return\n",
    "\n",
    "        if isinstance(event, FunctionToolCallEvent):\n",
    "            tool_name = event.part.tool_name\n",
    "            args = event.part.args\n",
    "            print(f\"TOOL CALL ({self.agent_name}): {tool_name}({args})\")\n",
    "\n",
    "    async def __call__(self, ctx, event):\n",
    "        return await self.print_function_calls(ctx, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7460b314-b66f-40b1-bd60-1f99c957b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = NamedCallback(search_agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de9e4e7a-2054-4298-bc5a-ab72d502c7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"build dashboards\"})\n",
      "TOOL CALL (search): search({\"query\": \"dashboard management\"})\n",
      "TOOL CALL (search): search({\"query\": \"add dashboard panels\"})\n",
      "TOOL CALL (search): get_file({\"filename\": \"docs/platform/dashboard_add_panels.mdx\"})\n",
      "TOOL CALL (search): get_file({\"filename\": \"docs/platform/dashboard_add_panels_ui.mdx\"})\n",
      "TOOL CALL (search): get_file({\"filename\": \"docs/platform/dashboard_panel_types.mdx\"})\n"
     ]
    }
   ],
   "source": [
    "result = await search_agent.run(\n",
    "    \"how do I build dashboads with evidently?\",\n",
    "    event_stream_handler=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1486b-2778-463a-a24c-3c546f3b68a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
