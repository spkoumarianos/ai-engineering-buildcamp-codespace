{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045f8a7f-9d2e-4708-b764-69a7e4b02235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m177 packages\u001b[0m \u001b[2min 4.59s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 252ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 53ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mminsearch\u001b[0m\u001b[2m==0.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mminsearch\u001b[0m\u001b[2m==0.0.9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add minsearch==0.0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524acdc1-c9b1-4a76-8ba5-24e4fcc39c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x203d337af90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    repo_name=\"docs\",\n",
    "    allowed_extensions={\"md\", \"mdx\"},\n",
    ")\n",
    "files = reader.read()\n",
    "\n",
    "parsed_docs = [doc.parse() for doc in files]\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "index.fit(parsed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0f91a7-9afe-4429-ba29-ac5b060e3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I use evidently to monitor my machine learning models?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a578376e-6f61-4d90-bfc8-a9f0c5974ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b15f74-0f99-46b3-ab05-dca20f98c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Highlighter, Tokenizer\n",
    "from minsearch.tokenizer import DEFAULT_ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a45d140-6216-45c5-8207-6202a75e0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = DEFAULT_ENGLISH_STOP_WORDS | {'evidently'}\n",
    "\n",
    "highlighter = Highlighter(\n",
    "    highlight_fields=['content'],\n",
    "    max_matches=3,\n",
    "    snippet_size=50,\n",
    "    tokenizer=Tokenizer(stemmer='snowball', stop_words=stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ff43877-05d5-4f4b-a0a2-e096c22f8034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'content': {'matches': ['...downloading and using ML **models** from HuggingFace. This l...',\n",
       "    '...criteria from the source **model**, e.g. classify texts by ...',\n",
       "    '.... There are:\\n\\n* Ready-to-**use** descriptors that wrap a ...'],\n",
       "   'total_matches': 33},\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'title': 'What is Evidently?',\n",
       "  'description': 'Welcome to the Evidently documentation.',\n",
       "  'mode': 'wide',\n",
       "  'content': {'matches': ['...elps evaluate, test, and **monitor** data and AI-powered syst...',\n",
       "    '...ucts: from predictive ML **models** to complex LLM-powered s...',\n",
       "    '.../Card>\\n  <Card title=\"ML **monitoring**\" icon=\"table\" href=\"/qui...'],\n",
       "   'total_matches': 4},\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'content': {'matches': ['...ucts: whether it’s an ML **model**, an LLM app, or a comple...',\n",
       "    '...workflow.\\n\\nOur tools are **model**-, framework-, and applic...',\n",
       "    '... It’s up to you.\\xa0\\n\\n* Add **monitoring** as you are ready to move...'],\n",
       "   'total_matches': 6},\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'title': 'Evidently Cloud',\n",
       "  'description': 'How to set up Evidently Cloud account.',\n",
       "  'icon': 'cloud',\n",
       "  'content': {'matches': ['... and Evidently Cloud v1, **use** `from evidently.ui.works...'],\n",
       "   'total_matches': 1},\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'title': 'Evidently Cloud v2',\n",
       "  'description': 'A new version of Evidently Cloud available starting April 10, 2025.',\n",
       "  'content': {'matches': ['...        |\\n\\nMake sure you **use** the matching version of ...'],\n",
       "   'total_matches': 1},\n",
       "  'filename': 'faq/cloud_v2.mdx'},\n",
       " {'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'content': {'matches': ['...llect telemetry when you **use** **Evidently **Monitoring** U...',\n",
       "    '...when you **use** **Evidently **Monitoring** UI**. We DO NOT collect ...',\n",
       "    '...t any telemetry when you **use** the tool as a library, f...'],\n",
       "   'total_matches': 7},\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'title': 'No code evals',\n",
       "  'description': 'How to evaluate your data in a no-code interface.',\n",
       "  'content': {'matches': ['... when you upload it.\\n- ****Use** an existing Dataset**. S...',\n",
       "    '...  **What are Datasets?** **Learn** how to manage and upload...',\n",
       "    '...u section if you plan to **use** an LLM for evaluations. ...'],\n",
       "   'total_matches': 14},\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'content': {'matches': ['... the UI Service to get a **monitoring** Dashboard and organize t...',\n",
       "    '...he snapshots and run the **monitoring** UI on the same **machine**.\\n...',\n",
       "    '...onitoring UI on the same **machine**.\\n\\nImports:\\n\\n```python\\nfr...'],\n",
       "   'total_matches': 7},\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'content': {'matches': [\"...e'll explore two ways to **use** an LLM as a judge:\\n\\n- **...\",\n",
       "    '....\\n- An OpenAI API key to **use** for the LLM evaluator.\\n\\n...',\n",
       "    '... \"ground truth\". You can **use** it to evaluate the quali...'],\n",
       "   'total_matches': 15},\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'title': 'Evidently and GitHub actions',\n",
       "  'description': 'Testing LLM outputs as part of the CI/CD flow.',\n",
       "  'content': {'matches': ['You can **use** Evidently together with ...',\n",
       "    '...an safely tweak prompts, **models**, or logic without breaki...'],\n",
       "   'total_matches': 2},\n",
       "  'filename': 'examples/GitHub_actions.mdx'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlighter.highlight(query, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8ef72be-c145-4d7b-b4cb-6e6501a26f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {doc['filename']: doc['content'] for doc in parsed_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5931fa5-c85a-4e27-9bb6-3a2894029388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Info>\\n  This page explains how to self-host the lightweight open-source platform. [Contact us](https://www.evidentlyai.com/get-demo) for Enterprise version with extra features and support. Compare [OSS vs. Enterprise/Cloud](/faq/oss_vs_cloud).\\n</Info>\\n\\nIn addition to using Evidently Python library, you can self-host the UI Service to get a monitoring Dashboard and organize the results of your evaluations. This is optional: you can also view evaluation results in Python or export to JSON or HTML.\\n\\nTo get a self-hostable Dashboard, you must:\\n\\n- Create a Workspace (local or remote) to store your data.\\n- Launch the UI service.\\n\\n## 1. Create a Workspace\\n\\n<Tip>\\n  Sign up for a free [Evidently Cloud](cloud) account to get a managed version instantly.\\n</Tip>\\n\\nOnce you [install Evidently](/docs/setup/installation), you will need to create a `workspace`. This designates a remote or local directory where you will store the evaluation results (JSON Reports called `snapshots`), traces or datasets. The UI Service will read the data from this source.\\n\\nAs a storage backend, Evidently supports:\\n\\n- a file system\\n- any SQL-like database (such as SQLite or Postgres)\\n- any S3-compatible storage (such as Amazon S3, GCS, or MinIO – through `fsspec`).\\n\\nThere are three scenarios, based on where you run the UI Service and store data.\\n\\n- **Local Workspace**. Both the UI Service and data storage are local.\\n- **Remote Workspace**. Both the UI Service and data storage are remote.\\n- **Workspace with remote data storage**. Run the UI Service and store data on different servers.\\n\\n### Local Workspace\\n\\nHere, you generate, store the snapshots and run the monitoring UI on the same machine.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a local Workspace and assign a name:\\n\\n```python\\nws = Workspace.create(\"evidently_ui_workspace\")\\n```\\n\\nYou can pass a `path` parameter to specify the path to a local directory.\\n\\n### Remote Workspace\\n\\n<Info>\\n  **Code example (Docker)**. See the [remote service example](https://github.com/evidentlyai/evidently/tree/main/examples/service).\\n</Info>\\n\\nIn this scenario, you send the snapshots to a remote server. You must run the Monitoring UI on the same remote server. It will directly interface with the filesystem where the snapshots are stored.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.remote import RemoteWorkspace\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a remote Workspace (UI should be running at this address):\\n\\n```python\\nworkspace = RemoteWorkspace(\"http://localhost:8000\")\\n```\\n\\nYou can pass the following parameters:\\n\\n| Parameter                  | Description                                                                                  |\\n| -------------------------- | -------------------------------------------------------------------------------------------- |\\n| `self.base_url = base_url` | URL for the remote UI service.                                                               |\\n| `self.secret = secret`     | String with secret, None by default. Use it if access to the URL is protected by a password. |\\n\\n### Remote snapshot storage\\n\\nIn the examples above, you store the snapshots and run the UI on the same server. Alternatively, you can store snapshots in a remote data store (such as an S3 bucket). The Monitoring UI service will interface with the designated data store to read the snapshot data.\\n\\nTo connect to data stores Evidently uses `fsspec` that allows accessing data on remote file systems via a standard Python interface.\\n\\nYou can verify supported data stores in the Fsspec documentation ([built-in implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations) and [other implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)).\\n\\nFor example, to read snapshots from an S3 bucket (with MinIO running on localhost:9000), you must specify environment variables:\\n\\n```\\nFSSPEC_S3_ENDPOINT_URL=http://localhost:9000/\\nFSSPEC_S3_KEY=my_key FSSPEC_S3_SECRET=my_secret\\nevidently ui --workspace s3://my_bucket/workspace\\n```\\n\\n### [DANGER] Delete Workspace\\n\\nTo delete a Workspace, run the command from the Terminal:\\n\\n```bash\\ncd src/evidently/ui/\\nrm -r workspace\\n```\\n\\n<Warning>\\n  **You are deleting all the data**. This command will delete all the data stored in the workspace folder. To maintain access to the generated Reports, you must store them elsewhere.\\n</Warning>\\n\\n## 2. Launch the UI service\\n\\nTo launch the Evidently UI service, you must run a command in the Terminal.\\n\\n**Option 1**. If you log snapshots to a local Workspace directory, you run Evidently UI over it. Run the following command from the directory where the Workspace folder is located.\\n\\n```bash\\nevidently ui\\n```\\n\\n**Option 2**. If you have your Project in a different Workspace, specify the path:\\n\\n```bash\\nevidently ui --workspace . /workspace\\n```\\n\\n**Option 3**. If you have your Project in a specified Workspace and run the UI service at the specific port (if the default port 8000 is occupied).\\n\\n```bash\\nevidently ui --workspace ./workspace --port 8080\\n```\\n\\nTo view the Evidently interface, go to URL http://localhost:8000 or a specified port in your web browser.\\n\\n### Demo projects\\n\\nTo launch the Evidently service with the demo projects, run:\\n\\n```\\nevidently ui --demo-projects all\\n```\\n\\n## Tutorials\\n\\n1. Check this tutorial for a simple end-to-end example:\\n\\n<Card title=\"Evidently UI tutorial\" icon=\"laptop-code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb\">\\n  How to create a workspace, project and run Reports.\\n</Card>\\n\\n2. Check this extended tutorial that shows LLM judge evaluation over collected traces and prompt optimization using a local workspace. There is also a [video walkthrough](https://youtu.be/Gem8TG6wNhU).\\n   \\n<Card title=\"Evidently UI tutorial\" icon=\"laptop-code\" href=\"https://github.com/evidentlyai/community-examples/blob/main/tutorials/local_content_generation.ipynb\">\\n  LLM evaluation for tweet generation using local workspace. \\n</Card>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_index['docs/setup/self-hosting.mdx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "026bad79-0bef-406e-8b33-e49812c67964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "class SearchTools:\n",
    "    \"\"\"\n",
    "    Provides search and file retrieval utilities over an indexed data store.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: Any,\n",
    "        highlighter: Any,\n",
    "        file_index: Dict[str, str]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SearchTools instance.\n",
    "\n",
    "        Args:\n",
    "            index: Searchable index providing a `search` method.\n",
    "            highlighter: Highlighter used to annotate search results.\n",
    "            file_index (Dict[str, str]): Mapping of filenames to file contents.\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.highlighter = highlighter\n",
    "        self.file_index = file_index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the index for results matching a query and highlight them.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query to look up in the index.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of highlighted search result objects.\n",
    "        \"\"\"\n",
    "        search_results = self.index.search(\n",
    "            query=query,\n",
    "            num_results=5\n",
    "        )\n",
    "\n",
    "        return self.highlighter.highlight(query, search_results)\n",
    "\n",
    "    def get_file(self, filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve a file's contents by filename.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the file to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The file contents if found, otherwise an error message.\n",
    "        \"\"\"\n",
    "        if filename in self.file_index:\n",
    "            return self.file_index[filename]\n",
    "        return f\"file {filename} does not exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2031f343-3a0b-4637-92fa-fff908cdf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index, highlighter, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "684bc3ef-47c1-44e3-a71b-6117a3368e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a documentation assistant.\n",
    "\n",
    "Answer the user question using only the documentation knowledge base.\n",
    "\n",
    "Make 3 iterations:\n",
    "\n",
    "1) First iteration:\n",
    "   - Perform one search using the search tool to identify potentially relevant documents.\n",
    "   - Explain (in 2–3 sentences) why this search query is appropriate for the user question.\n",
    "\n",
    "2) Second iteration:\n",
    "   - Analyze the results from the previous search.\n",
    "   - Based on the filenames or documents returned, perform:\n",
    "       - Up to 2 additional search queries to refine or expand coverage, and\n",
    "       - One or more get_file calls to retrieve the full content of the most relevant documents.\n",
    "   - For each search or get_file call, explain (in 2–3 sentences) why this action is necessary and how it helps answer the question.\n",
    "\n",
    "3) Third iteration:\n",
    "   - Analyze the retrieved document contents from get_file.\n",
    "   - Synthesize the information from these documents into a final answer to the user.\n",
    "\n",
    "IMPORTANT:\n",
    "- At every step, explicitly explain your reasoning for each search query or file retrieval.\n",
    "- Use only facts found in the documentation knowledge base.\n",
    "- Do not introduce outside knowledge or assumptions.\n",
    "- If the answer cannot be found in the retrieved documents, clearly inform the user.\n",
    "\n",
    "Additional notes:\n",
    "- The knowledge base is entirely about Evidently, so you do not need to include the word \"evidently\" in search queries.\n",
    "- Prefer retrieving and analyzing full documents (via get_file) before producing the final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50bb0ab8-0cd9-47ff-82bd-5c6b85aefb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'get_file',\n",
       "  'description': 'No description provided.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'filename': {'type': 'string',\n",
       "     'description': 'filename parameter'}},\n",
       "   'required': ['filename'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'No description provided.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toyaikit.tools import Tools\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tools(search_tools)\n",
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19f4eecf-fde7-4c9d-b93e-190c6261271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from toyaikit.llm import OpenAIClient\n",
    "\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.interface import IPythonChatInterface\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b0fe7e3-8aed-4fbc-ad8e-8d263134d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAIClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    client=OpenAI()\n",
    ")\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "runner_callback = DisplayingRunnerCallback(chat_interface=chat_interface) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea50782a-03c1-4ccb-af3d-cce032948a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=llm_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6290f0e5-25fc-4186-b11e-ada2c3309797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"monitor machine learning models code\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"monitor machine learning models code\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"title\": \"No code evals\",\n",
       "    \"description\": \"How to evaluate your data in a no-code interface.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...  **What are Datasets?** **Learn** how to manage and upload...\",\n",
       "        \"... following methods:\\n\\n- ****Model**-based**: use built-in **machine**...\",\n",
       "        \"...el-based**: use built-in **machine** **learning** **models**, like se...\"\n",
       "      ],\n",
       "      \"total_matches\": 7\n",
       "    },\n",
       "    \"filename\": \"docs/platform/evals_no_code.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Use HuggingFace models\",\n",
       "    \"description\": \"How to use models from HuggingFace as evaluators.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...downloading and using ML **models** from HuggingFace. This l...\",\n",
       "        \"...criteria from the source **model**, e.g. classify texts by ...\",\n",
       "        \"...ors that wrap a specific **model**,\\n\\n* A general interface ...\"\n",
       "      ],\n",
       "      \"total_matches\": 26\n",
       "    },\n",
       "    \"filename\": \"metrics/customize_hf_descriptor.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Tutorials and guides\",\n",
       "    \"description\": \"End-to-end code examples.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...LLM evaluation methods. [**CODE** \\\\+ VIDEO]\\n  &lt;/Card&gt;\\n  &lt;C...\",\n",
       "        \"... optimization (2)\\\" icon=\\\"**code**\\\" href=\\\"https://github.co...\",\n",
       "        \"...e=\\\"GitHub actions\\\" icon=\\\"**code**\\\" href=\\\"/examples/GitHub_...\"\n",
       "      ],\n",
       "      \"total_matches\": 13\n",
       "    },\n",
       "    \"filename\": \"examples/introduction.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Synthetic data generation\",\n",
       "    \"description\": \"[NEW] Code-first synthetic data generation.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...thetic data generation: [**code** example](https://github....\"\n",
       "      ],\n",
       "      \"total_matches\": 1\n",
       "    },\n",
       "    \"filename\": \"docs/library/synthetic_data_api.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"All Descriptors\",\n",
       "    \"description\": \"Reference page for all row-level text and LLM evals.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...rt_llm). For a reference **code** example, see this [Descr...\",\n",
       "        \"...ms=[\\\"as a large language **model**\\\"])`&lt;/li&gt;&lt;/ul&gt;           ...\",\n",
       "        \"...ructured data formats or **code** syntax.\\n\\n| Name         ...\"\n",
       "      ],\n",
       "      \"total_matches\": 25\n",
       "    },\n",
       "    \"filename\": \"metrics/all_descriptors.mdx\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The search for &quot;monitor machine learning models code&quot; has returned several relevant documents that may include examples or tutorials on how to monitor machine learning models using the system. This is appropriate because the user is looking specifically for code-related content to assist with monitoring models effectively.</p>\n",
       "<h3>Next Steps:</h3>\n",
       "<ol>\n",
       "<li>I'll search specifically for more code examples or tutorials that focus on monitoring machine learning models.</li>\n",
       "<li>I'll also look for documents about tutorials and guides that might contain relevant code snippets.</li>\n",
       "</ol>\n",
       "<p>Let's proceed with both actions.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"monitoring models code examples\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"monitoring models code examples\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"title\": \"Use HuggingFace models\",\n",
       "    \"description\": \"How to use models from HuggingFace as evaluators.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...downloading and using ML **models** from HuggingFace. This l...\",\n",
       "        \"...criteria from the source **model**, e.g. classify texts by ...\",\n",
       "        \"...ors that wrap a specific **model**,\\n\\n* A general interface ...\"\n",
       "      ],\n",
       "      \"total_matches\": 31\n",
       "    },\n",
       "    \"filename\": \"metrics/customize_hf_descriptor.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"No code evals\",\n",
       "    \"description\": \"How to evaluate your data in a no-code interface.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"... following methods:\\n\\n- ****Model**-based**: use built-in ma...\",\n",
       "        \"...uilt-in machine learning **models**, like sentiment analysis...\",\n",
       "        \"...).\\n&lt;/Note&gt;\\n\\nHere are few **examples** of Descriptors and how t...\"\n",
       "      ],\n",
       "      \"total_matches\": 7\n",
       "    },\n",
       "    \"filename\": \"docs/platform/evals_no_code.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Tutorials and guides\",\n",
       "    \"description\": \"End-to-end code examples.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...LM Tutorials\\n\\nEnd-to-end **examples** of specific workflows an...\",\n",
       "        \"...=\\\"scale-balanced\\\" href=\\\"/**examples**/LLM_judge\\\"&gt;\\n    How to c...\",\n",
       "        \"...n\\\" icon=\\\"comment\\\" href=\\\"/**examples**/LLM_rag_evals\\\"&gt;\\n    A wa...\"\n",
       "      ],\n",
       "      \"total_matches\": 34\n",
       "    },\n",
       "    \"filename\": \"examples/introduction.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Dashboard panel types [Legacy]\",\n",
       "    \"description\": \"Overview of the available monitoring Panels.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...space v1. \\n\\n## What is a **monitoring** Panel?\\n\\nA **monitoring** Pan...\",\n",
       "        \"...s a **monitoring** Panel?\\n\\nA **monitoring** Panel is an individual p...\",\n",
       "        \"...l plot or counter on the **Monitoring** Dashboard.\\u00a0\\n\\n* You can a...\"\n",
       "      ],\n",
       "      \"total_matches\": 23\n",
       "    },\n",
       "    \"filename\": \"docs/platform/dashboard_panel_types.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Overview\",\n",
       "    \"description\": \"How production AI quality monitoring works.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...s several ways to set up **monitoring**.\\n\\n## Batch **monitoring** jo...\",\n",
       "        \"...up **monitoring**.\\n\\n## Batch **monitoring** jobs\\n\\n&lt;Check&gt;\\n  Supporte...\",\n",
       "        \"...ur infrastructure to run **monitoring** jobs. This can be a Pyth...\"\n",
       "      ],\n",
       "      \"total_matches\": 12\n",
       "    },\n",
       "    \"filename\": \"docs/platform/monitoring_overview.mdx\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"model monitoring tutorials\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"model monitoring tutorials\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"title\": \"Dashboard panel types [Legacy]\",\n",
       "    \"description\": \"Overview of the available monitoring Panels.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...space v1. \\n\\n## What is a **monitoring** Panel?\\n\\nA **monitoring** Pan...\",\n",
       "        \"...s a **monitoring** Panel?\\n\\nA **monitoring** Panel is an individual p...\",\n",
       "        \"...l plot or counter on the **Monitoring** Dashboard.\\u00a0\\n\\n* You can a...\"\n",
       "      ],\n",
       "      \"total_matches\": 9\n",
       "    },\n",
       "    \"filename\": \"docs/platform/dashboard_panel_types.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Overview\",\n",
       "    \"description\": \"How production AI quality monitoring works.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...s several ways to set up **monitoring**.\\n\\n## Batch **monitoring** jo...\",\n",
       "        \"...up **monitoring**.\\n\\n## Batch **monitoring** jobs\\n\\n&lt;Check&gt;\\n  Supporte...\",\n",
       "        \"...ur infrastructure to run **monitoring** jobs. This can be a Pyth...\"\n",
       "      ],\n",
       "      \"total_matches\": 11\n",
       "    },\n",
       "    \"filename\": \"docs/platform/monitoring_overview.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Manage Projects\",\n",
       "    \"description\": \"Set up an evaluation or monitoring Project.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...the Project ID above the **monitoring** Dashboard inside your Pr...\",\n",
       "        \"...ng description or adding **monitoring** Panels), always use the ...\",\n",
       "        \"...vailable Projects on the **monitoring** homepage, or request a l...\"\n",
       "      ],\n",
       "      \"total_matches\": 6\n",
       "    },\n",
       "    \"filename\": \"docs/platform/projects_manage.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Tutorials and guides\",\n",
       "    \"description\": \"End-to-end code examples.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...e video course with 10\\\\+ **tutorials**. [Sign up](https://www.e...\",\n",
       "        \"...rd&gt;\\n&lt;/CardGroup&gt;\\n\\n## LLM **Tutorials**\\n\\nEnd-to-end examples of ...\",\n",
       "        \"...ard&gt;\\n&lt;/CardGroup&gt;\\n\\n## ML **tutorials**\\n\\nEnd-to-end examples of ...\"\n",
       "      ],\n",
       "      \"total_matches\": 15\n",
       "    },\n",
       "    \"filename\": \"examples/introduction.mdx\"\n",
       "  },\n",
       "  {\n",
       "    \"title\": \"Batch monitoring\",\n",
       "    \"description\": \"How to run batch evaluation jobs.\",\n",
       "    \"content\": {\n",
       "      \"matches\": [\n",
       "        \"...g_flow_batch.png)\\n\\nBatch **monitoring** relies on the core evalu...\",\n",
       "        \"...igure your own choice of **monitoring** Panels. Check the [Dashb...\",\n",
       "        \"...as Tests - as opposed to **monitoring** lots of metrics at once ...\"\n",
       "      ],\n",
       "      \"total_matches\": 3\n",
       "    },\n",
       "    \"filename\": \"docs/platform/monitoring_local_batch.mdx\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>get_file({\"filename\":\"docs/platform/monitoring_overview....)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"filename\":\"docs/platform/monitoring_overview.mdx\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>\"AI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This gives an up-to-date view of your system behavior and helps spot and fix issues.\\n\\nEvidently offers several ways to set up monitoring.\\n\\n## Batch monitoring jobs\\n\\n&lt;Check&gt;\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n&lt;/Check&gt;\\n\\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\\u2019t need instant quality evaluations.\\n\\n![](/images/monitoring_flow_batch.png)\\n\\n**How it works**:\\n\\n* **Build your evaluation pipeline**. Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow. Run it at regular intervals (e.g., hourly, daily) or trigger it when new data or labels arrive.\\n\\n* **Run metric calculations**. Implement the evaluation step in the pipeline using the Evidently Python library. Select the evals, and compute the `Reports` that will summarize data, metrics, and test results.\\n\\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\\n\\n**Benefits of this approach**:\\n\\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\\u2019re already stored elsewhere, like for retraining.\\n\\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\\n\\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection, naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\\n\\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\\n\\n## Tracing with scheduled evals\\n\\n&lt;Check&gt;\\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\\n&lt;/Check&gt;\\n\\n**Best for**: LLM-powered applications\\n\\n![](/images/monitoring_flow_tracing.png)\\n\\n**How it works:**\\n\\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\\n\\n* **Store raw data**. Evidently Platform stores all raw data, providing a complete record of activity.\\n\\n* **Schedule evaluations**. Set up evaluations to run automatically at scheduled times. This will generate Reports or run Tests directly on the Evidently Platform. You can also manually run evaluations anytime to assess individual outputs.\\n\\n**Benefits of this approach**:\\n\\n* **Solves the data capture**. You collect complex traces and all production data in one place, making it easier to manage and analyze.\\n\\n* **Easy to re-run evals**. With raw traces stored on the platform, you can easily re-run evaluations or add new metrics whenever needed.\\n\\n* **No-code**. Once your trace instrumentation is set up, you can manage everything from the UI.\\n\\n**Next step:** check the [Tracing Quickstart](/quickstart_tracing).\"</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>get_file({\"filename\":\"docs/platform/dashboard_panel_type...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"filename\":\"docs/platform/dashboard_panel_types.mdx\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>\"This applies to versions 0.6.0 to 0.7.1 for Cloud/Workspace v1. \\n\\n## What is a monitoring Panel?\\n\\nA monitoring Panel is an individual plot or counter on the Monitoring Dashboard.\\u00a0\\n\\n* You can add multiple Panels and organize them by **Tabs**. You can customize Panel type, values shown, titles and legends.\\n\\n* When adding a Panel, you choose a **Test** or **Metric** with the specific value (\\\"metric result\\\") inside it. Evidently pulls corresponding value(s) from all Reports in the Project and plots them on the Panel.\\n\\n* You can use **Tags** to filter data from specific Reports. For example, you can plot the accuracy of Model A and Model B next to each other. To achieve this, [add relevant Tags](/docs/library/tags_metadata) to the Report.\\n\\n&lt;Info&gt;\\n  **How to add Panels**. This page explains the Panel types. Check the next section on [adding Panels](/docs/platform/dashboard_add_panels).\\n&lt;/Info&gt;\\n\\n**Panel types.** There are 3 main panel types:\\n\\n* **Metric panels** plot individual values from inside Reports.\\n\\n* **Test panels** show pass/fail Test outcomes in time.\\n\\n* **Distribution panels** plot distributions over time.\\n\\n## Metric Panels\\n\\nMetric Panels (`DashboardPanel`) show individual values from inside the Reports in time.\\n\\nFor example, if you capture Data Summary Reports (include mean, max, min, etc., for each column) or Data Drift Reports (include the share of drifting columns and per-column drift score), you can plot any of these values in time.\\n\\n![](/images/dashboard/metric_panels.png)\\n\\n**Panel time resolution** depends on Report frequency. For instance, if you log Data Drift Reports daily, you can plot the share of drifting features with daily granularity. You can also open the source Report to see feature distributions on a specific day.\\n\\n### Counter\\n\\nClass `DashboardPanelCounter`.\\n\\nShows a value with supporting text or text alone. Perfect for dashboard titles.\\n\\n&lt;Frame&gt;\\n  &lt;img src=\\\"/images/dashboard/panel_counter_example-min.png\\\" /&gt;\\n&lt;/Frame&gt;\\n\\n### Plot\\n\\nClass `DashboardPanelPlot`. Shows individual values as bar, line, scatter plot, or histogram.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Line chart\\\" img=\\\"/images/dashboard//panel_line_plot_example.png\\\"&gt;\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Bar chart\\\" img=\\\"/images/dashboard//panel_bar_plot_example.png\\\"&gt;\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Scatter plot\\\" img=\\\"/images/dashboard//panel_scatter_plot_example.png\\\"&gt;\\n    `PlotType.SCATTER` shows values over time from multiple Reports.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Histogram\\\" img=\\\"/images/dashboard//panel_hist_example.png\\\"&gt;\\n    `PlotType.HISTOGRAM` shows the frequency of individual values across Reports.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## Test Panels\\n\\nTest Panels show the Test results.\\n\\nAs you run the same Tests repeatedly, you can visualize the pass/fail outcomes or result counts. You choose which Test results to include.\\n\\n![](/images/dashboard/test_panels.png)\\n\\nTest Panels only work with Test Suites: you must add Tests to the Metrics inside your Report to be able to render these panels.\\n\\n### Test counter\\n\\nClass `DashboardPanelTestSuiteCounter`. Shows a counter of Tests with specified status.\\n\\n&lt;Frame&gt;\\n  &lt;img src=\\\"/images/dashboard/panel_tests_counter_example.png\\\" /&gt;\\n&lt;/Frame&gt;\\n\\n### Test plot\\n\\nClass `DashboardPanelTestSuite`.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Detailed plot\\\" img=\\\"/images/dashboard/panel_tests_detailed_hover_example.png\\\"&gt;\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Aggregated plot\\\" img=\\\"/images/dashboard/panel_tests_aggregated_hover_example.png\\\"&gt;\\n    `TestSuitePanelType.AGGREGATE`. Only the total number of Tests by status is visible.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## Distribution Panel\\n\\nClass `DashboardPanelDistribution`. Shows a distribution of values over time.\\n\\n![](/images/dashboard//distribution_panels.png)\\n\\nFor example, if you capture Text Evals or Data Summary that include histograms for categorical values, you can plot how the frequency of categories changes.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Stacked\\\" img=\\\"/images/dashboard/panel_dist_stacked_2-min.png\\\"&gt;\\n    `barmode=\\\"stack\\\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Grouped\\\" img=\\\"/images/dashboard/panel_dist_group_2-min.png\\\"&gt;\\n    `barmode=\\\"group\\\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Overlay\\\" img=\\\"/images/dashboard/panel_dist_overlay-min.png\\\"&gt;\\n    `barmode=\\\"overlay\\\"`: overlay bar chart shows overlaying **absolute** counts.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Relative\\\" img=\\\"/images/dashboard/panel_dist_relative-min.png\\\"&gt;\\n    `barmode=\\\"relative\\\"`: relative bar chart shows stacked **relative** frequency.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n&lt;Info&gt;\\n  **What is the difference between a Distribution panel and a Histogram?** A histogram plot (`DashboardPanelPlot` with`PlotType.HISTOGRAM`) shows the distribution of the selected values from all Reports. Each source Report contains a **single value** (e.g., a \\\"mean value\\\"). A Distribution Panel (`DashboardPanelDistribution`) shows how a distribution changes over time. Each source Report contains a **histogram** (e.g. frequency of different categories).\\n&lt;/Info&gt;\\n\\n## What's next?\\n\\nHow to add [monitoring Panels and Tabs](/docs/platform/dashboard_add_panels).\\n\\n---\\ntitle: 'Add dashboard panels'\\ndescription: 'How to design your Dashboard with custom Panels.'\\n---\\n\\nThis page shows how to add panels one by one. Check [pre-built Tabs](/docs/platform/dashboard_tabs) for a quick start, and explore [available Panel types](/docs/platform/dashboard_panel_types).\\n\\n## Adding Tabs\\n\\n&lt;Check&gt;\\n  Multiple Tabs are available in the Evidently Cloud and Enterprise.\\n&lt;/Check&gt;\\n\\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\\n\\n**User interface.** Enter the \\\"Edit\\\" mode on the Dashboard (top right corner) and click the plus sign with \\\"add Tab\\\". To create a custom Tab, choose an \\u201cempty\\u201d tab and give it a name.\\n\\n**Python**. You can add an empty tab using `create_tab`:\\n\\n```python\\nproject.dashboard.create_tab(\\\"My tab\\\")\\nproject.save()\\n```\\n\\nYou can also use the `add_panel` method shown below and specify the destination Tab. If there is no Tab with a set name, you will create both a new Tab and Panel at once. If it already exists, a new Panel will appear below others in this Tab.\\n\\n## Adding Panels\\n\\nYou can add Panels in the user interface or using Python API.\\n\\n### User interface\\n\\n&lt;Check&gt;\\n  No-code Dashboards are available in the Evidently Cloud and Enterprise.\\n&lt;/Check&gt;\\n\\nOnce you are inside the Project:\\n\\n* Enter the \\\"Edit\\\" mode by clicking on the top right corner of the Dashboard.\\n\\n* Click on the \\\"Add panel\\\" button.\\n\\n* Follow the flow to configure dashboard name, type, etc.\\n\\n* Preview and publish.\\n\\nTo delete/edit a Panel, enter Edit mode and hover over a specific Panel to choose an action.\\n\\n### Python API\\n\\n&lt;Check&gt;\\n  Dashboards as code are available in Evidently OSS, Cloud, Enterprise.\\n&lt;/Check&gt;\\n\\n&lt;Tip&gt;\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) (or your [local workspace](/docs/setup/self-hosting)) and [create a Project](/docs/platform/projects_manage).\\n&lt;/Tip&gt;\\n\\nImport the necessary modules to configure the Panels as code:\\n\\n```python\\nfrom evidently.future.metrics import *\\nfrom evidently.ui.dashboards import DashboardPanelCounter\\nfrom evidently.ui.dashboards import DashboardPanelDistribution\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import TestFilter\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nHere is the general flow to add a new Panel:\\n\\n&lt;Steps&gt;\\n  &lt;Step title=\\\"Connect to the Project\\\"&gt;\\n    Load the latest dashboard configuration into your Python environment.\\n\\n    ```python\\n    project = ws.get_project(\\\"YOUR PROJECT ID HERE\\\")\\n    ```\\n  &lt;/Step&gt;\\n\\n  &lt;Step title=\\\"Add a new Panel\\\"&gt;\\n    Use the `add_panel` method and configure the Panel:\\n\\n    * Pick the [Panel type](/docs/platform/dashboard_panel_types): Counter, Plot, Distribution, Test Counter, Test Plot.\\n\\n    * Set applicable Panel **parameters.** (See below for each type).\\n\\n    * Specify Panel **title** and **size**.\\n\\n    * Add optional **Tags** to filter data. If empty, the Panel will use data from all Reports.\\n\\n    * Define what the Panel will show (see examples below):\\n\\n      * Use `values` to point a specific Metric result, or\\n\\n      * Use `test_filters` to select Tests.\\n\\n    * Set if the Panel should appear on specific **Tab**.\\n\\n    For example, to add a line plot that shows Row Count in time to the \\\"Overview\\\" tab:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\\\"Row count\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\\\"metric.metric_id\\\": RowCount().metric_id},\\n                        field_path=\\\"value\\\",\\n                        legend=\\\"count\\\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\\\"Overview\\\"\\n        )\\n    project.save()\\n    ```\\n\\n    You can add multiple Panels at once: they will appear in the listed order.\\n  &lt;/Step&gt;\\n\\n  &lt;Step title=\\\"Save\\\"&gt;\\n    Save the configuration with `project.save()`. Go back to the web app to see the Dashboard. Refresh the page if needed.\\n  &lt;/Step&gt;\\n&lt;/Steps&gt;\\n\\n\\n**Delete Panels.** To delete all monitoring Panels, use:\\n\\n```\\nproject.dashboard.panels = []\\n\\nproject.save()\\n```\\n\\n&lt;Info&gt;\\n  **Note**: This does not delete the Reports or data; it only deletes the Panel configuration.\\n&lt;/Info&gt;\\n\\n## Panel Parameters\\n\\n### General parameters\\n\\nClass `DashboardPanel` is a base class. These parameters apply to all Panel types.\\n\\n| Parameter              | Example use                                              | Description                                                                                                                                                                      |\\n| ---------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `title: str`           | `title=\\\"My Panel\\\"`                                       | Panel name visible at the header.                                                                                                                                                |\\n| `filter: ReportFilter` | `filter=ReportFilter(metadata_values={}, tag_values=[])` | Filters define a subset of Reports from which to display the data. Tags or metadata values you list must be added when logging Reports. See [docs](/docs/library/tags_metadata). |\\n| `size: WidgetSize`     | `size=WidgetSize.HALF`, `size=WidgetSize.FULL` (default) | Sets the Panel size to half-width or full-sized.                                                                                                                                 |\\n\\n### Counter\\n\\n`DashboardPanelCounter` shows a value count or works as a text-only Panel.\\n\\n&lt;Frame&gt;\\n  &lt;img src=\\\"/images/dashboard/panel_counter_example-min.png\\\" /&gt;\\n&lt;/Frame&gt;\\n\\nExamples usage:\\n\\n&lt;Tabs&gt;\\n  &lt;Tab title=\\\"Text Panel\\\"&gt;\\n    **Text only panel**. To create a Panel with the Dashboard title only:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\\\"LLM chatbot monitoring\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                agg=CounterAgg.NONE,\\n                size=WidgetSize.FULL,\\n            ),\\n            tab=\\\"Overview\\\"\\n        )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Value sum\\\"&gt;\\n    **Value sum**. To create a Panel that sums up the number of rows over time:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\\\"Model calls\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\\\"metric.metric_id\\\": RowCount().metric_id},\\n                    field_path=\\\"value\\\",\\n                    legend=\\\"count\\\",\\n                ),\\n                text=\\\"count\\\",\\n                agg=CounterAgg.SUM,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Last value\\\"&gt;\\n    **Last value**. To create a Panel that shows the number of rows in the last Report.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\\\"Row number: last run\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\\\"metric.metric_id\\\": RowCount().metric_id},\\n                    field_path=\\\"value\\\",\\n                    legend=\\\"count\\\",\\n                ),\\n                text=\\\"count\\\",\\n                agg=CounterAgg.LAST,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n&lt;/Tabs&gt;\\n\\nAll parameters:\\n\\n| Parameter                     | Description                                                                                                                                                                                                                                                   |\\n| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: Optional[PanelValue]` | Specifies the value to display. &lt;br /&gt; &lt;br /&gt;You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Check the Panel Value section below for details.* &lt;br /&gt;&lt;br /&gt;If left empty, displays a text-only panel. |\\n| `text: Optional[str]`         | Supporting text to display.                                                                                                                                                                                                                                   |\\n| `agg: CounterAgg`             | Data aggregation options: &lt;br /&gt;`SUM`: Calculates the value sum. &lt;br /&gt;`LAST`: Shows the last available value. &lt;br /&gt;`NONE`: Reserved for text panels.                                                                                                        |\\n\\n### Plot\\n\\n`DashboardPanelPlot` shows individual values over time.&#x20;\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Line chart\\\" img=\\\"/images/dashboard//panel_line_plot_example.png\\\"&gt;\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Bar chart\\\" img=\\\"/images/dashboard//panel_bar_plot_example.png\\\"&gt;\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\nExample usage:\\n\\n&lt;Tabs&gt;\\n  &lt;Tab title=\\\"Single value\\\"&gt;\\n    **Single value**. To plot row count as a LINE plot (you can change to BAR etc.):\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\\\"Row count\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\\\"metric.metric_id\\\": RowCount().metric_id},\\n                        field_path=\\\"value\\\",\\n                        legend=\\\"count\\\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\\\"Overview\\\"\\n        )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Multiple values\\\"&gt;\\n    **Multiple values.** To plot min, max and mean values of the \\\"length\\\" column on the same plot:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\\\"Length\\\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\\\"metric.metric_id\\\": MinValue(column=\\\"length\\\").metric_id},\\n                        field_path=\\\"value\\\",\\n                        legend=\\\"min\\\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\\\"metric.metric_id\\\": MeanValue(column=\\\"length\\\").metric_id},\\n                        field_path=\\\"value\\\",\\n                        legend=\\\"mean\\\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\\\"metric.metric_id\\\": MaxValue(column=\\\"length\\\").metric_id},\\n                        field_path=\\\"value\\\",\\n                        legend=\\\"max\\\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\\\"Overview\\\"\\n        )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n&lt;/Tabs&gt;\\n\\nAll parameters:\\n\\n| Parameter                  | Description                                                                                                                                                                                                                                                                                                                                                                                |\\n| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `values: List[PanelValue]` | Specifies the value(s) to display in the Plot. &lt;br /&gt;&lt;br /&gt; You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Refer to the Panel Value section below for details.* &lt;br /&gt;&lt;br /&gt; You can pass multiple values so that will appear together, e.g., as separate lines on a Line plot, bars on a Bar Chart, or points on a Scatter Plot. |\\n| `plot_type: PlotType`      | Specifies the plot type. &lt;br /&gt;&lt;br /&gt;**Available:** `SCATTER`, `BAR`, `LINE`, `HISTOGRAM`                                                                                                                                                                                                                                                                                                  |\\n\\n### Distribution\\n\\n`DashboardPanelDistribution` shows changes in the distribution over time. It's mostly relevant for showing distributions of categorical columns.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Stacked\\\" img=\\\"/images/dashboard/panel_dist_stacked_2-min.png\\\"&gt;\\n    `barmode=\\\"stack\\\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Grouped\\\" img=\\\"/images/dashboard/panel_dist_group_2-min.png\\\"&gt;\\n    `barmode=\\\"group\\\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n**Example**. To plot the distribution of the column \\\"refusals\\\" that contains binary labels:\\n\\n```python\\nproject.dashboard.add_panel(\\n        DashboardPanelDistribution(\\n            title=\\\"Is the context valid? (group)\\\",\\n            value=PanelValue(\\n                field_path=\\\"counts\\\", \\n                metric_args={\\\"metric.metric_id\\\": UniqueValueCount(column=\\\"context quality\\\").metric_id}\\n                ),\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            barmode=\\\"group\\\",\\n            size=WidgetSize.FULL,\\n        ),\\n    tab=\\\"Overview\\\"\\n    )\\nproject.save()\\n```\\n\\nAll parameters:\\n\\n| Parameter              | Description                                                                                                                                                                 |\\n| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: PanelValue`    | Specifies the distribution to display on the Panel. &lt;br /&gt;&lt;br /&gt; You must point to a named **Metric** that contains a distribution histogram and set `field_path=\\\"counts\\\"`. |\\n| `barmode: HistBarMode` | Specifies the distribution plot type. &lt;br /&gt;&lt;br /&gt;**Available:** `stack`, `group`, `overlay`, `relative`                                                                    |\\n\\n### Test Counter\\n\\n`DashboardPanelTestSuiteCounter` shows a counter with Test results.\\n\\n&lt;Frame&gt;\\n  &lt;img src=\\\"/images/dashboard/panel_tests_counter_example.png\\\" /&gt;\\n&lt;/Frame&gt;\\n\\nExample usage:\\n\\n&lt;Tabs&gt;\\n  &lt;Tab title=\\\"All Tests (Last result)\\\"&gt;\\n    **All Tests**. To display the results of the latest Test Suite. Filter by LAST, no filter on Test name.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\\\"Latest Test suite: results\\\",\\n            agg=CounterAgg.LAST,\\n        ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n\\n    All parameters:\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Specific Test (All time)\\\"&gt;\\n    **Specific Test**. To show all failures of a specific Test over time. (Filtered by status and Test name).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\\\"Empty Rows Test (Total Failed)\\\",\\n            test_filters=[\\n                TestFilter(test_args={\\\"test.metric_fingerprint\\\": EmptyRowsCount().metric_id})\\n                ],\\n            statuses=[TestStatus.FAIL]\\n        ),\\n        tab=\\\"Tests\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Specific Test (Last result)\\\"&gt;\\n    **Specific Test**. To show the last result of a specific Test. (Defaults to showing success status).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\\\"Empty Rows Test (Last result)\\\",\\n            test_filters=[\\n                TestFilter(test_args={\\\"test.metric_fingerprint\\\": EmptyRowsCount().metric_id})\\n                ],\\n            agg=CounterAgg.LAST\\n        ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n&lt;/Tabs&gt;\\n\\n| Parameter                                                                                                                                                                  | Description                                                                                                                                                                |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]=[]`                                                                                                                                        | Test filters select specific Test(s). Without a filter, the Panel considers the results of all Tests.                                                                      |\\n| `statuses: List[statuses]`&lt;br /&gt;&lt;br /&gt;**Available**:&lt;br /&gt;`TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS`(default), `TestStatus.WARNING`, `TestStatus.SKIPPED` | Status filters select Tests with specific outcomes. (E.g., choose the FAIL status to display a counter for failed Tests). Without a filter, the Panel shows Tests SUCCESS. |\\n| &lt;br /&gt;`agg: CounterAgg`&lt;br /&gt;&lt;br /&gt;**Available**:&lt;br /&gt;`SUM`(default),  `LAST`                                                                                             | Data aggregation options:&lt;br /&gt;`SUM`: Calculates the sum of Test results. &lt;br /&gt;`LAST`: Displays the last available Test result.                                           |\\n\\n### Test Plot\\n\\n`DashboardPanelTestSuite` shows Test results over time.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Detailed plot\\\" img=\\\"/images/dashboard/panel_tests_detailed_hover_example.png\\\"&gt;\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  &lt;/Card&gt;\\n\\n  &lt;Card title=\\\"Aggregated plot\\\" img=\\\"/images/dashboard/panel_tests_aggregated_hover_example.png\\\"&gt;\\n    `TestSuitePanelType.AGGREGATE`. Only the total number of Tests by status is visible.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\nExample usage:\\n\\n&lt;Tabs&gt;\\n  &lt;Tab title=\\\"Detailed (All Tests)\\\"&gt;\\n    **All Tests**. Show the results of all Tests in the Project with per-Test granularity.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\\\"All tests: detailed\\\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.DETAILED\\n        ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Detailed (Selected Tests)\\\"&gt;\\n    **Selected Tests**. Show the results of selected Tests in the Project with per-Test granularity.\\n\\n    ```\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\\\"Selected tests (missing)\\\",\\n            test_filters=[\\n                TestFilter(test_args={\\\"test.metric_fingerprint\\\": MissingValueCount(column=\\\"length\\\").metric_id}),\\n                TestFilter(test_args={\\\"test.metric_fingerprint\\\": MissingValueCount(column=\\\"sentiment\\\").metric_id})\\n            ],\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.HALF,\\n            panel_type=TestSuitePanelType.DETAILED,\\n        ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n\\n  &lt;Tab title=\\\"Aggregated (All Tests)\\\"&gt;\\n    **All Tests.** Show the results of all Tests in aggregate (Total passed/fail).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\\\"All tests: detailed\\\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.AGGREGATE\\n        ),\\n        tab=\\\"Overview\\\"\\n    )\\n    project.save()\\n    ```\\n  &lt;/Tab&gt;\\n&lt;/Tabs&gt;\\n\\nAll parameters:\\n\\n| Parameter                                                                                                                                                                                          | Description                                                                                                                                                                                                                        |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]`                                                                                                                                                                   | Test filters select specific Test(s). Without a filter, the Panel shows the results of all Tests.                                                                                                                                  |\\n| `statuses: List[statuses]`&lt;br /&gt;&lt;br /&gt;**Available**:&lt;br /&gt;`TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS`, `TestStatus.WARNING`, `TestStatus.SKIPPED`                                  | Status filters select Tests with specific outcomes. By default the Panel shows all Test statuses.                                                                                                                                  |\\n| `panel_type=TestSuitePanelType`&lt;br /&gt;&lt;br /&gt;**Available**:&lt;br /&gt;`TestSuitePanelType.DETAILED`&lt;br /&gt;`TestSuitePanelType.AGGREGATE`                                                                   | Defines the Panel type. **Detailed** shows individual Test results where you can hover and see individual results and click to open a corresponding Test Suite. **Aggregate** (default) shows the total number of Tests by status. |\\n| `time_agg: Optional[str] = None`&lt;br /&gt;&lt;br /&gt;**Available**:&lt;br /&gt;`1H`, `1D`, `1W`, `1M` (see [period aliases](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-period-aliases)) | Groups all Test results in a period (e.g., 1 DAY).                                                                                                                                                                                 |\\n\\n## Panel Value\\n\\n**Metric ID**. To point to the Metric or Test to plot on a Panel, you use  `test_filters` or `metric_args `as shown above and pass `metric_id` or `metric_fingerprint` . They must include the name of the Metric that was logged to the Project. You must use the same Metic name (with any applicable parameters) that you used when creating the Report.\\n\\n&lt;Note&gt;\\n  **Working with Presets.** You must reference a named Evidently Metric even if you used a Preset. You can check the Metrics included in each Preset [here](/metrics/all_metrics).\\n&lt;/Note&gt;\\n\\n**Field path**. For Metric Panels, you also specify the `field_path`. This helps point to a specific **result** inside the Metric. This can take the following values: `value` , `share`/`count` or `values` .\\n\\n| Field path         | Description                                          | Applicable Metrics                                                               | Applicable Panels |\\n| ------------------ | ---------------------------------------------------- | -------------------------------------------------------------------------------- | ----------------- |\\n| `value`            | Points to a single result from the Metric.           | Most Metrics                                                                     | Counter, Plot     |\\n| `share` or `count` | Points to either absolute count or percentage value. | Metrics that return both absolute and percentage values like `MissingValueCount` | Counter, Plot     |\\n| `shares` or `counts`        | Points to a histogram visualization within a Metric. | Metrics with histogram visualizations,  like `UniqueValueCount`.                 | Distribution      |\\n\\nThere are a few exceptions where a Metric can return a different result or a dictionary.\\n\\n&lt;Note&gt;\\n  **How to verify the result of a specific Metric?** Check in the [All Metrics table](/metrics/all_metrics). You can also generate the Report with a given Metric, export the Report as JSON and check the value name it returns.\\n&lt;/Note&gt;\\n\\nWhen working in the Evidently Cloud, you can see available fields in the drop-down menu as you add a new Panel.\"</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>get_file({\"filename\":\"docs/platform/monitoring_local_bat...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"filename\":\"docs/platform/monitoring_local_batch.mdx\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>\"Read the overview of the approach [here](/docs/platform/monitoring_overview).\\n\\n![](/images/monitoring_flow_batch.png)\\n\\nBatch monitoring relies on the core evaluation API of the Evidently Python library. Check the [detailed guide](/docs/library/evaluations_overview).\\n\\n## Simple Example\\n\\n&lt;Tip&gt;\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\\n&lt;/Tip&gt;\\n\\nTo get the dataset stats for a single batch and upload to the workspace:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DatasetStats()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n## Workflow\\n\\nThe complete workflow looks as the following.\\n\\n&lt;Steps&gt;\\n  &lt;Step title=\\\"Configure the metrics\\\"&gt;\\n    Define an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to define the evals.\\n  &lt;/Step&gt;\\n  &lt;Step title=\\\"Run the evals\\\"&gt;\\n    You must independently execute Reports on a chosen cadence. Consider tools like Airflow. You can send Reports from different steps in your pipeline. For example:\\n\\n    - first, send data quality, data drift and prediction drift checks\\n    - after you get the delayed labels, send a ML quality checks results.\\n\\n    You can backdate your Reports with a custom timestamp. &lt;img src=\\\"/images/monitoring_batch_workflow_min.png\\\" alt=\\\"\\\" /&gt;\\n  &lt;/Step&gt;\\n  &lt;Step title=\\\"Upload to the platform\\\"&gt;\\n    Choose to store raw inferences or only upload the metric summaries. [How to upload / delete results](/docs/platform/evals_api).\\n  &lt;/Step&gt;\\n  &lt;Step title=\\\"Configure the Dashboard\\\"&gt;\\n    Set up a Dashboard to track results over time: using pre-built Tabs or configure your own choice of monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\\n  &lt;/Step&gt;\\n  &lt;Step title=\\\"Configure alerts\\\"&gt;\\n    Set up alerts on Metric values or Test failures. Check the section on [Alerts](/docs/platform/alerts).\\n  &lt;/Step&gt;\\n&lt;/Steps&gt;\\n\\n&lt;Tip&gt;\\n  **Running Tests vs Reports**. Structuring your evaluations as Tests - as opposed to monitoring lots of metrics at once - can help reduce alert fatigue and simplify configuration when evaluating multiple conditions at once. For example, you can quickly verify that all columns in the input data are within a defined min-max range.\\n&lt;/Tip&gt;\"</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>get_file({\"filename\":\"examples/introduction.mdx\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"filename\":\"examples/introduction.mdx\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>\"&lt;Note&gt;\\n  **We have an applied course on LLM evaluations\\\\!** Free video course with 10\\\\+ tutorials. [Sign up](https://www.evidentlyai.com/llm-evaluation-course-practice).\\n&lt;/Note&gt;\\n\\n## Quickstarts\\n\\nIf you are new, start here.\\n\\n&lt;CardGroup cols={3}&gt;\\n  &lt;Card title=\\\"LLM quickstart\\\" icon=\\\"comment-text\\\" href=\\\"/quickstart_llm\\\"&gt;\\n    Evaluate the quality of text outputs.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"ML quickstart\\\" icon=\\\"table\\\" href=\\\"/quickstart_ml\\\"&gt;\\n    Test tabular data quality and data drift.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"Tracing quickstart\\\" icon=\\\"bars-staggered\\\" href=\\\"/quickstart_tracing\\\"&gt;\\n    Collect inputs and outputs from AI your app.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## LLM Tutorials\\n\\nEnd-to-end examples of specific workflows and use cases.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"LLM as a judge\\\" icon=\\\"scale-balanced\\\" href=\\\"/examples/LLM_judge\\\"&gt;\\n    How to create and evaluate an LLM judge against human labels.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"RAG evaluation\\\" icon=\\\"comment\\\" href=\\\"/examples/LLM_rag_evals\\\"&gt;\\n    A walkthrough of different RAG evaluation metrics.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"LLM as a jury\\\" icon=\\\"dice\\\" href=\\\"LLM_jury\\\"&gt;\\n    Using multiple LLMs to evaluate the same output.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"LLM evaluation methods\\\" icon=\\\"text\\\" href=\\\"LLM_evals\\\"&gt;\\n    A walkthrough of different LLM evaluation methods. [CODE \\\\+ VIDEO]\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"Descriptor cookbook\\\" icon=\\\"book\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb\\\"&gt;\\n    A walkthrough of different descriptors (deterministic, ML, etc.) a single notebook.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"LLM judge prompt optimization (1)\\\" icon=\\\"hotel\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb\\\"&gt;\\n    Optimize a multi-class classifier using target labels.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"LLM judge prompt optimization (2)\\\" icon=\\\"code\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb\\\"&gt;\\n    Optimize a binary classifier using target labels and free-form feedback.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## ML tutorials\\n\\nEnd-to-end examples of specific workflows and use cases.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Metric cookbook\\\" icon=\\\"book\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb\\\"&gt;\\n    Various data/ML metrics: Regression, Classification, Data Quality, Data Drift.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## Integrations\\n\\nEnd-to-end examples of integrating Evidently with other tools and platforms.\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"GitHub actions\\\" icon=\\\"code\\\" href=\\\"/examples/GitHub_actions\\\"&gt;\\n    Running Evidently evals as part of CI/CD workflow. Native GitHub action integration for regression testing.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"Different LLM providers as judges\\\" icon=\\\"sparkles\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb\\\"&gt;\\n    Examples of using different external evaluator LLMs as LLM judges: OpenAI, Gemini, Google Vertex, Mistral, Ollama.\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"Evidently + Grafana: LLM evals\\\" icon=\\\"chart-gantt\\\" href=\\\"https://github.com/evidentlyai/evidently/tree/main/examples/llm_eval_grafana_dashboard\\\"&gt;\\n    Visualize Evidently LLM evaluation metrics with Grafana. (Postgres as a database).\\n  &lt;/Card&gt;\\n  &lt;Card title=\\\"Evidently+ Grafana: Data drift\\\" icon=\\\"chart-column\\\" href=\\\"https://github.com/evidentlyai/evidently/tree/main/examples/data_drift_grafana_dashboard\\\"&gt;\\n    Visualize Evidently data drift evaluations on a Grafana dashboard. (Postgres as a database).\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n\\n## Deployment\\n\\n&lt;CardGroup cols={2}&gt;\\n  &lt;Card title=\\\"Evidently Open-source UI tutorial\\\" icon=\\\"laptop-code\\\" href=\\\"https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb\\\"&gt;\\n    How to create a workspace, project and run Reports.\\n  &lt;/Card&gt;\\n&lt;/CardGroup&gt;\\n\\n## LLM Evaluation Course - Video Tutorials\\n\\nWe have an applied LLM evaluation course where we walk through the core evaluation workflows. Each consists of the code example and a video tutorial walthrough.\\n\\n\\ud83d\\udce5 [Sign up for the course](https://www.evidentlyai.com/llm-evaluation-course-practice)\\n\\n\\ud83d\\udcf9 [See complete Youtube playlist](https://www.youtube.com/watch?v=K8LLVi5Xrh8&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=2)\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Code example**                                                                                                                         | **Video**                                                                                                                                 |\\n| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\\n| **Intro to LLM Evals**           | Introduction to LLM evaluation: concepts, goals, and motivations behind evaluating LLM outputs.                                                                                                                                                                                                                                                                                                                                                            | \\u2013                                                                                                                                        | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. &lt;ul&gt;        &lt;li&gt;        Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.&lt;/li&gt;                &lt;li&gt;        Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.&lt;/li&gt;                &lt;li&gt;        Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.&lt;/li&gt;                &lt;/ul&gt; | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | &lt;ul&gt;        &lt;li&gt;        Video 1&lt;/li&gt;                &lt;li&gt;        Video 2&lt;/li&gt;                &lt;li&gt;        Video 3&lt;/li&gt;                &lt;/ul&gt; |\\n| **LLM as a Judge**               | Tutorial on creating and tuning LLM judges aligned with human preferences.                                                                                                                                                                                                                                                                                                                                                                                 | [Open Notebook](LLMCourse_Tutorial_2_LLM_as_a_judge.ipynb)                                                                               | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n| **Clasification Evaluation**     | Tutorial on evaluating LLMs and a simple predictive ML baseline on a multi-class classification task.                                                                                                                                                                                                                                                                                                                                                      | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Classification_Evals.ipynb)                  | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n| **Content Generation with LLMs** | Tutorial on how to use LLMs to write tweets and evaluate how engaging they are. Introduction to the concept of tracing.                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Content_Generation_Evals.ipynb)              | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n| **RAG evaluations**              | &lt;ul&gt;        &lt;li&gt;        Part 1. Theory on how to evaluate RAG systems: retrieval, generation quality and synthetic data.&lt;/li&gt;                &lt;li&gt;        Part 2. Tutorial on building a toy RAG application and evaluating correctness and faithfulness.&lt;/li&gt;                &lt;/ul&gt;                                                                                                                                                                         | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_RAG_Evals.ipynb)                             | &lt;ul&gt;        &lt;li&gt;        Video 1&lt;/li&gt;                &lt;li&gt;        Video 2&lt;/li&gt;                &lt;/ul&gt;                                         |\\n| **AI agent evaluations**         | Tutorial on how to build a simple Q&A agent and evaluate tool choice and answer correctness.                                                                                                                                                                                                                                                                                                                                                               | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Agent_Evals.ipynb)                           | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n| **Adversarial testing**          | Tutorial on how to run scenario-based risk testing on forbidden topics and brand risks.                                                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Adversarial_Testing.ipynb)                   | &lt;ul&gt;        &lt;li&gt;        Video&lt;/li&gt;                &lt;/ul&gt;                                                                                   |\\n\\n## More examples\\n\\nYou can also find more examples in the [Example Repository](https://github.com/evidentlyai/community-examples).\"</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><h3>Final Answer</h3>\n",
       "<p>To monitor your machine learning models using Evidently, you can set up batch monitoring and create various dashboard panels. Here’s an overview of how you can implement this:</p>\n",
       "<ol>\n",
       "<li><p><strong>Set Up Data Evaluation Pipeline</strong>:\n",
       "To get started, you'll need to evaluate your dataset using a simple Python script:</p>\n",
       "<pre><code class=\"language-python\">import pandas as pd\n",
       "from evidently.metrics import DatasetStats\n",
       "from evidently.report import Report\n",
       "from evidently.data import Dataset\n",
       "\n",
       "# Prepare your data\n",
       "eval_data = Dataset.from_pandas(pd.DataFrame(source_df))\n",
       "\n",
       "# Create a report\n",
       "report = Report([DatasetStats()])\n",
       "\n",
       "# Run the evaluation\n",
       "my_eval = report.run(eval_data, None)\n",
       "</code></pre>\n",
       "<p>This code evaluates the dataset and creates a report with selected metrics.</p>\n",
       "</li>\n",
       "<li><p><strong>Configure Batch Jobs</strong>:\n",
       "You can set up batch jobs to regularly run your monitoring evaluations. Here’s a basic workflow:</p>\n",
       "<ul>\n",
       "<li><strong>Configure Your Metrics</strong>: Define an Evidently report with any tests and metrics you want to monitor.</li>\n",
       "<li><strong>Run the Evals</strong>: Schedule your evaluations using a tool like Airflow.</li>\n",
       "<li><strong>Store and Visualize Results</strong>: Upload your report runs to a designated workspace and visualize the metrics on a dashboard.</li>\n",
       "</ul>\n",
       "<p>Example of how to upload results:</p>\n",
       "<pre><code class=\"language-python\">ws.add_run(project.id, my_eval, include_data=False)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create Dashboard Panels</strong>:\n",
       "Use the following code to create various types of panels in your dashboard to visualize metrics and test results:</p>\n",
       "<pre><code class=\"language-python\">from evidently.ui import DashboardPanelPlot, PanelValue, WidgetSize, PlotType\n",
       "\n",
       "project.dashboard.add_panel(\n",
       "    DashboardPanelPlot(\n",
       "        title=&quot;Example Metric&quot;,\n",
       "        values=[PanelValue(\n",
       "            metric_args={&quot;metric.metric_id&quot;: YourMetric().metric_id},\n",
       "            field_path=&quot;value&quot;,\n",
       "            legend=&quot;example&quot;\n",
       "        )],\n",
       "        plot_type=PlotType.LINE,\n",
       "        size=WidgetSize.HALF\n",
       "    ),\n",
       "    tab=&quot;Overview&quot;\n",
       ")\n",
       "\n",
       "project.save()\n",
       "</code></pre>\n",
       "<p>You can adjust this code to create counter panels for tests, aggregate results, or show detailed results over time by selecting the appropriate panel type.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Key Features:</h3>\n",
       "<ul>\n",
       "<li><strong>Batch Monitoring</strong>: Great for periodic evaluations without real-time need (check the batch monitoring docs for details on setup).</li>\n",
       "<li><strong>Dashboards</strong>: Customizable dashboard panels to visualize metrics, historic performance, and test statuses.</li>\n",
       "</ul>\n",
       "<p>For full details, including setup instructions, refer to the comprehensive programming guides in Evidently's official documentation.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = agent.loop(\n",
    "    query + ' show me the code',\n",
    "    callback=runner_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05b87b-a4e0-4400-b504-315ec9f20473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
